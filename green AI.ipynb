{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b4c165f",
      "metadata": {
        "id": "4b4c165f"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "class Vgg16_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Vgg16_net, self).__init__()\n",
        "\n",
        "        self.layer1=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,stride=1,padding=1), #(32-3+2)/1+1=32   32*32*64\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), #(32-3+2)/1+1=32    32*32*64\n",
        "            \n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2)   \n",
        "        )\n",
        "\n",
        "        self.layer2=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1),  #(16-3+2)/1+1=16  16*16*128\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), #(16-3+2)/1+1=16   16*16*128\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(2,2)    \n",
        "        )\n",
        "\n",
        "        self.layer3=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1),  #(8-3+2)/1+1=8   8*8*256\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1),  #(8-3+2)/1+1=8   8*8*256\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1),  #(8-3+2)/1+1=8   8*8*256\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(2,2)     \n",
        "        )\n",
        "\n",
        "        self.layer4=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=1,padding=1),  #(4-3+2)/1+1=4    4*4*512\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1),   #(4-3+2)/1+1=4    4*4*512\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1),   #(4-3+2)/1+1=4    4*4*512\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(2,2)    \n",
        "        )\n",
        "\n",
        "        self.layer5=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1),   #(2-3+2)/1+1=2    2*2*512\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1),  #(2-3+2)/1+1=2     2*2*512\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1),  #(2-3+2)/1+1=2      2*2*512\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(2,2)   \n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv=nn.Sequential(\n",
        "            self.layer1,\n",
        "            self.layer2,\n",
        "            self.layer3,\n",
        "            self.layer4,\n",
        "            self.layer5\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9cc497",
      "metadata": {
        "id": "df9cc497"
      },
      "outputs": [],
      "source": [
        "vgg16=Vgg16_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d1d89e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9d1d89e",
        "outputId": "5bd95aee-724f-446c-fba3-8435c0eb1191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "            Conv2d-2         [-1, 64, 224, 224]           1,792\n",
            "       BatchNorm2d-3         [-1, 64, 224, 224]             128\n",
            "       BatchNorm2d-4         [-1, 64, 224, 224]             128\n",
            "              ReLU-5         [-1, 64, 224, 224]               0\n",
            "              ReLU-6         [-1, 64, 224, 224]               0\n",
            "            Conv2d-7         [-1, 64, 224, 224]          36,928\n",
            "            Conv2d-8         [-1, 64, 224, 224]          36,928\n",
            "       BatchNorm2d-9         [-1, 64, 224, 224]             128\n",
            "      BatchNorm2d-10         [-1, 64, 224, 224]             128\n",
            "             ReLU-11         [-1, 64, 224, 224]               0\n",
            "             ReLU-12         [-1, 64, 224, 224]               0\n",
            "        MaxPool2d-13         [-1, 64, 112, 112]               0\n",
            "        MaxPool2d-14         [-1, 64, 112, 112]               0\n",
            "           Conv2d-15        [-1, 128, 112, 112]          73,856\n",
            "           Conv2d-16        [-1, 128, 112, 112]          73,856\n",
            "      BatchNorm2d-17        [-1, 128, 112, 112]             256\n",
            "      BatchNorm2d-18        [-1, 128, 112, 112]             256\n",
            "             ReLU-19        [-1, 128, 112, 112]               0\n",
            "             ReLU-20        [-1, 128, 112, 112]               0\n",
            "           Conv2d-21        [-1, 128, 112, 112]         147,584\n",
            "           Conv2d-22        [-1, 128, 112, 112]         147,584\n",
            "      BatchNorm2d-23        [-1, 128, 112, 112]             256\n",
            "      BatchNorm2d-24        [-1, 128, 112, 112]             256\n",
            "             ReLU-25        [-1, 128, 112, 112]               0\n",
            "             ReLU-26        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-27          [-1, 128, 56, 56]               0\n",
            "        MaxPool2d-28          [-1, 128, 56, 56]               0\n",
            "           Conv2d-29          [-1, 256, 56, 56]         295,168\n",
            "           Conv2d-30          [-1, 256, 56, 56]         295,168\n",
            "      BatchNorm2d-31          [-1, 256, 56, 56]             512\n",
            "      BatchNorm2d-32          [-1, 256, 56, 56]             512\n",
            "             ReLU-33          [-1, 256, 56, 56]               0\n",
            "             ReLU-34          [-1, 256, 56, 56]               0\n",
            "           Conv2d-35          [-1, 256, 56, 56]         590,080\n",
            "           Conv2d-36          [-1, 256, 56, 56]         590,080\n",
            "      BatchNorm2d-37          [-1, 256, 56, 56]             512\n",
            "      BatchNorm2d-38          [-1, 256, 56, 56]             512\n",
            "             ReLU-39          [-1, 256, 56, 56]               0\n",
            "             ReLU-40          [-1, 256, 56, 56]               0\n",
            "           Conv2d-41          [-1, 256, 56, 56]         590,080\n",
            "           Conv2d-42          [-1, 256, 56, 56]         590,080\n",
            "      BatchNorm2d-43          [-1, 256, 56, 56]             512\n",
            "      BatchNorm2d-44          [-1, 256, 56, 56]             512\n",
            "             ReLU-45          [-1, 256, 56, 56]               0\n",
            "             ReLU-46          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-47          [-1, 256, 28, 28]               0\n",
            "        MaxPool2d-48          [-1, 256, 28, 28]               0\n",
            "           Conv2d-49          [-1, 512, 28, 28]       1,180,160\n",
            "           Conv2d-50          [-1, 512, 28, 28]       1,180,160\n",
            "      BatchNorm2d-51          [-1, 512, 28, 28]           1,024\n",
            "      BatchNorm2d-52          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-53          [-1, 512, 28, 28]               0\n",
            "             ReLU-54          [-1, 512, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]       2,359,808\n",
            "           Conv2d-56          [-1, 512, 28, 28]       2,359,808\n",
            "      BatchNorm2d-57          [-1, 512, 28, 28]           1,024\n",
            "      BatchNorm2d-58          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-59          [-1, 512, 28, 28]               0\n",
            "             ReLU-60          [-1, 512, 28, 28]               0\n",
            "           Conv2d-61          [-1, 512, 28, 28]       2,359,808\n",
            "           Conv2d-62          [-1, 512, 28, 28]       2,359,808\n",
            "      BatchNorm2d-63          [-1, 512, 28, 28]           1,024\n",
            "      BatchNorm2d-64          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-65          [-1, 512, 28, 28]               0\n",
            "             ReLU-66          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-67          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-68          [-1, 512, 14, 14]               0\n",
            "           Conv2d-69          [-1, 512, 14, 14]       2,359,808\n",
            "           Conv2d-70          [-1, 512, 14, 14]       2,359,808\n",
            "      BatchNorm2d-71          [-1, 512, 14, 14]           1,024\n",
            "      BatchNorm2d-72          [-1, 512, 14, 14]           1,024\n",
            "             ReLU-73          [-1, 512, 14, 14]               0\n",
            "             ReLU-74          [-1, 512, 14, 14]               0\n",
            "           Conv2d-75          [-1, 512, 14, 14]       2,359,808\n",
            "           Conv2d-76          [-1, 512, 14, 14]       2,359,808\n",
            "      BatchNorm2d-77          [-1, 512, 14, 14]           1,024\n",
            "      BatchNorm2d-78          [-1, 512, 14, 14]           1,024\n",
            "             ReLU-79          [-1, 512, 14, 14]               0\n",
            "             ReLU-80          [-1, 512, 14, 14]               0\n",
            "           Conv2d-81          [-1, 512, 14, 14]       2,359,808\n",
            "           Conv2d-82          [-1, 512, 14, 14]       2,359,808\n",
            "      BatchNorm2d-83          [-1, 512, 14, 14]           1,024\n",
            "      BatchNorm2d-84          [-1, 512, 14, 14]           1,024\n",
            "             ReLU-85          [-1, 512, 14, 14]               0\n",
            "             ReLU-86          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-87            [-1, 512, 7, 7]               0\n",
            "        MaxPool2d-88            [-1, 512, 7, 7]               0\n",
            "================================================================\n",
            "Total params: 29,446,272\n",
            "Trainable params: 29,446,272\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 643.51\n",
            "Params size (MB): 112.33\n",
            "Estimated Total Size (MB): 756.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import pandas as pd\n",
        "vgg16=Vgg16_net().to(device)\n",
        "summary(vgg16,(3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1e23eb",
      "metadata": {
        "id": "1d1e23eb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class Bottleneck(nn.Module):\n",
        "\n",
        "    extention=4\n",
        "    def __init__(self,inplanes,planes,stride,downsample=None):\n",
        "        \n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv1=nn.Conv2d(inplanes,planes,kernel_size=1,stride=stride,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv2=nn.Conv2d(planes,planes,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "        self.bn2=nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv3=nn.Conv2d(planes,planes*self.extention,kernel_size=1,stride=1,bias=False)\n",
        "        self.bn3=nn.BatchNorm2d(planes*self.extention)\n",
        "\n",
        "        self.relu=nn.ReLU(inplace=True)\n",
        "\n",
        "        \n",
        "        self.downsample=downsample\n",
        "        self.stride=stride\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual=x\n",
        "\n",
        "        out=self.conv1(x)\n",
        "        out=self.bn1(out)\n",
        "        out=self.relu(out)\n",
        "\n",
        "        out=self.conv2(out)\n",
        "        out=self.bn2(out)\n",
        "        out=self.relu(out)\n",
        "\n",
        "        out=self.conv3(out)\n",
        "        out=self.bn3(out)\n",
        "        out=self.relu(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual=self.downsample(x)\n",
        "\n",
        "        out+=residual\n",
        "        out=self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self,block,layers,num_class):\n",
        "        self.inplane=64\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.block=block\n",
        "        self.layers=layers\n",
        "\n",
        "        self.conv1=nn.Conv2d(3,self.inplane,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(self.inplane)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "\n",
        "        self.stage1=self.make_layer(self.block,64,layers[0],stride=1)\n",
        "        self.stage2=self.make_layer(self.block,128,layers[1],stride=2)\n",
        "        self.stage3=self.make_layer(self.block,256,layers[2],stride=2)\n",
        "        self.stage4=self.make_layer(self.block,512,layers[3],stride=2)\n",
        "\n",
        "        self.avgpool=nn.AvgPool2d(7)\n",
        "        self.fc=nn.Linear(512*block.extention,num_class)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=self.conv1(x)\n",
        "        out=self.bn1(out)\n",
        "        out=self.relu(out)\n",
        "        out=self.maxpool(out)\n",
        "\n",
        "        out=self.stage1(out)\n",
        "        out=self.stage2(out)\n",
        "        out=self.stage3(out)\n",
        "        out=self.stage4(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def make_layer(self,block,plane,block_num,stride=1):\n",
        "        block_list=[]\n",
        "        downsample=None\n",
        "        if(stride!=1 or self.inplane!=plane*block.extention):\n",
        "            downsample=nn.Sequential(\n",
        "                nn.Conv2d(self.inplane,plane*block.extention,stride=stride,kernel_size=1,bias=False),\n",
        "                nn.BatchNorm2d(plane*block.extention)\n",
        "            )\n",
        "\n",
        "        conv_block=block(self.inplane,plane,stride=stride,downsample=downsample)\n",
        "        block_list.append(conv_block)\n",
        "        self.inplane=plane*block.extention\n",
        "\n",
        "        for i in range(1,block_num):\n",
        "            block_list.append(block(self.inplane,plane,stride=1))\n",
        "\n",
        "        return nn.Sequential(*block_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "resnet=ResNet(Bottleneck,[3,4,6,3],1000).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184a1329",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "184a1329",
        "outputId": "c1fda43f-9061-4437-a74f-1c886ab89342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "             ReLU-13          [-1, 256, 56, 56]               0\n",
            "           Conv2d-14          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-15          [-1, 256, 56, 56]             512\n",
            "             ReLU-16          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-17          [-1, 256, 56, 56]               0\n",
            "           Conv2d-18           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "             ReLU-20           [-1, 64, 56, 56]               0\n",
            "           Conv2d-21           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
            "             ReLU-23           [-1, 64, 56, 56]               0\n",
            "           Conv2d-24          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-25          [-1, 256, 56, 56]             512\n",
            "             ReLU-26          [-1, 256, 56, 56]               0\n",
            "             ReLU-27          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-28          [-1, 256, 56, 56]               0\n",
            "           Conv2d-29           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
            "             ReLU-31           [-1, 64, 56, 56]               0\n",
            "           Conv2d-32           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "             ReLU-34           [-1, 64, 56, 56]               0\n",
            "           Conv2d-35          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-36          [-1, 256, 56, 56]             512\n",
            "             ReLU-37          [-1, 256, 56, 56]               0\n",
            "             ReLU-38          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-39          [-1, 256, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]          32,768\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
            "             ReLU-45          [-1, 128, 28, 28]               0\n",
            "           Conv2d-46          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-47          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-50          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-51          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-52          [-1, 512, 28, 28]               0\n",
            "           Conv2d-53          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "             ReLU-55          [-1, 128, 28, 28]               0\n",
            "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
            "             ReLU-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-61          [-1, 512, 28, 28]               0\n",
            "             ReLU-62          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-63          [-1, 512, 28, 28]               0\n",
            "           Conv2d-64          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "             ReLU-66          [-1, 128, 28, 28]               0\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "             ReLU-69          [-1, 128, 28, 28]               0\n",
            "           Conv2d-70          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-71          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-72          [-1, 512, 28, 28]               0\n",
            "             ReLU-73          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-74          [-1, 512, 28, 28]               0\n",
            "           Conv2d-75          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 128, 28, 28]             256\n",
            "             ReLU-77          [-1, 128, 28, 28]               0\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "             ReLU-80          [-1, 128, 28, 28]               0\n",
            "           Conv2d-81          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-82          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-83          [-1, 512, 28, 28]               0\n",
            "             ReLU-84          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-85          [-1, 512, 28, 28]               0\n",
            "           Conv2d-86          [-1, 256, 14, 14]         131,072\n",
            "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
            "             ReLU-88          [-1, 256, 14, 14]               0\n",
            "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
            "             ReLU-91          [-1, 256, 14, 14]               0\n",
            "           Conv2d-92         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-93         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-94         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-95         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-96         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-97         [-1, 1024, 14, 14]               0\n",
            "       Bottleneck-98         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-99          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-100          [-1, 256, 14, 14]             512\n",
            "            ReLU-101          [-1, 256, 14, 14]               0\n",
            "          Conv2d-102          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-103          [-1, 256, 14, 14]             512\n",
            "            ReLU-104          [-1, 256, 14, 14]               0\n",
            "          Conv2d-105         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-106         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-107         [-1, 1024, 14, 14]               0\n",
            "            ReLU-108         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-109         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-110          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-111          [-1, 256, 14, 14]             512\n",
            "            ReLU-112          [-1, 256, 14, 14]               0\n",
            "          Conv2d-113          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-114          [-1, 256, 14, 14]             512\n",
            "            ReLU-115          [-1, 256, 14, 14]               0\n",
            "          Conv2d-116         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-117         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-118         [-1, 1024, 14, 14]               0\n",
            "            ReLU-119         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
            "            ReLU-123          [-1, 256, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "            ReLU-130         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-131         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-132          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-133          [-1, 256, 14, 14]             512\n",
            "            ReLU-134          [-1, 256, 14, 14]               0\n",
            "          Conv2d-135          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-136          [-1, 256, 14, 14]             512\n",
            "            ReLU-137          [-1, 256, 14, 14]               0\n",
            "          Conv2d-138         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-139         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-140         [-1, 1024, 14, 14]               0\n",
            "            ReLU-141         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-142         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-143          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-144          [-1, 256, 14, 14]             512\n",
            "            ReLU-145          [-1, 256, 14, 14]               0\n",
            "          Conv2d-146          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-147          [-1, 256, 14, 14]             512\n",
            "            ReLU-148          [-1, 256, 14, 14]               0\n",
            "          Conv2d-149         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-150         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-151         [-1, 1024, 14, 14]               0\n",
            "            ReLU-152         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-153         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-154            [-1, 512, 7, 7]         524,288\n",
            "     BatchNorm2d-155            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-156            [-1, 512, 7, 7]               0\n",
            "          Conv2d-157            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-158            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-159            [-1, 512, 7, 7]               0\n",
            "          Conv2d-160           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-161           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-162           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-163           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-164           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-165           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-166           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-167            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-168            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-169            [-1, 512, 7, 7]               0\n",
            "          Conv2d-170            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-171            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-172            [-1, 512, 7, 7]               0\n",
            "          Conv2d-173           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-174           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-175           [-1, 2048, 7, 7]               0\n",
            "            ReLU-176           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-177           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-178            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-179            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-180            [-1, 512, 7, 7]               0\n",
            "          Conv2d-181            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-182            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-183            [-1, 512, 7, 7]               0\n",
            "          Conv2d-184           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-185           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-186           [-1, 2048, 7, 7]               0\n",
            "            ReLU-187           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-188           [-1, 2048, 7, 7]               0\n",
            "================================================================\n",
            "Total params: 23,508,032\n",
            "Trainable params: 23,508,032\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 316.59\n",
            "Params size (MB): 89.68\n",
            "Estimated Total Size (MB): 406.84\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(resnet,(3,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c5d625",
      "metadata": {
        "id": "91c5d625"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self,in_channel,out_channel,k,s,p,bias=False):\n",
        "        super(ConvBlock,self).__init__()\n",
        "        self.conv =nn.Sequential( nn.Conv2d(\n",
        "        in_channel,out_channel,kernel_size=k,stride=s, padding=p,bias=bias),\n",
        "        nn.BatchNorm2d(out_channel),\n",
        "        nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "class InceptionV3base(nn.Module):\n",
        "    def __init__(self, in_channel,layers=[64,48,64,64,96,96,32] ):\n",
        "        super(InceptionV3base,self).__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channel,layers[0],kernel_size=1,stride=1,padding=0 ,bias=False),\n",
        "        nn.BatchNorm2d(layers[0]),\n",
        "        nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channel,layers[1],1,stride=1,padding=0,bias=False),\n",
        "        nn.BatchNorm2d(layers[1]),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(layers[1],layers[2],5,stride=1,padding=2,bias=False),\n",
        "        nn.BatchNorm2d(layers[2]),\n",
        "        nn.ReLU(True),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channel,layers[3],kernel_size=1,stride=1,padding=0,bias=False),\n",
        "        nn.BatchNorm2d(layers[3]),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(layers[3],layers[4],kernel_size=3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(layers[4]),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(layers[4],layers[5],kernel_size=3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(layers[5]),\n",
        "        nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "        nn.AvgPool2d(kernel_size=3,stride=1,padding=1),\n",
        "        nn.Conv2d(in_channel,layers[6],1,stride=1,padding=0,bias=False),\n",
        "        nn.BatchNorm2d(layers[6]),\n",
        "        nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "        b4 = self.branch4(x)\n",
        "        return torch.cat([b1,b2,b3,b4],dim=1)\n",
        "class InceptionV3_discount(nn.Module):\n",
        "    def __init__(self,in_channel,layers=[384,64,96,96],block=ConvBlock):\n",
        "        super(InceptionV3_discount,self).__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "        block(in_channel,layers[0],3,2,0),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "        block(in_channel,layers[1],1,1,0),\n",
        "        block(layers[1],layers[2],3,1,1),\n",
        "        block(layers[2],layers[3],3,2,0),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "        nn.MaxPool2d(3,2,padding=0),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "        return torch.cat([b1,b2,b3],dim=1)\n",
        "\n",
        "class InceptionV3Block(nn.Module):\n",
        "    def __init__(self,in_channel,layers=[192,128,128,192,128,128,128,128,192,192],block=ConvBlock):\n",
        "        super(InceptionV3Block,self).__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "        block(in_channel,layers[0],1,1,0),)\n",
        "        self.branch2 = nn.Sequential(\n",
        "        block(in_channel,layers[1],1,1,0),\n",
        "        block(layers[1],layers[2],(1,7),1,(0,3)),\n",
        "        block(layers[2],layers[3],(7,1),1,(3,0)),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            block(in_channel,layers[4],1,1,0),\n",
        "            block(layers[4],layers[5],(7,1),1,(3,0)),\n",
        "            block(layers[5],layers[6],(1,7),1,(0,3)),\n",
        "            block(layers[6],layers[7],(7,1),1,(3,0)),\n",
        "            block(layers[7],layers[8],(1,7),1,(0,3)),\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "        nn.AvgPool2d(3,1,padding=1),\n",
        "        block(in_channel,layers[9],1,1,0),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "        b4 = self.branch4(x)\n",
        "        return torch.cat([b1,b2,b3,b4],dim=1)\n",
        "\n",
        "class InceptionV3Substraction(nn.Module):\n",
        "    def __init__(self,in_channel,layers=[192,320,192,192,192,192],block=ConvBlock):\n",
        "        super(InceptionV3Substraction,self).__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "        block(in_channel,layers[0],1,1,0),\n",
        "        block(layers[0],layers[1],3,2,0),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "        block(in_channel,layers[2],1,1,0),\n",
        "        block(layers[2],layers[3],(1,7),1,(0,3)),\n",
        "        block(layers[3],layers[4],(7,1),1,(3,0)),\n",
        "        block(layers[4],layers[5],(3,3),2,0),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "        nn.MaxPool2d(3,stride=2,padding=0),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "        return torch.cat([b1,b2,b3],dim=1)\n",
        "class Concat(nn.Module):\n",
        "    def __init__(self,in_channel,layers=[384,384],block=ConvBlock):\n",
        "        super(Concat,self).__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "        block(in_channel,layers[0],(1,3),1,(0,1)),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "        block(in_channel,layers[1],(3,1),1,(1,0)),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        return torch.cat([b1,b2],dim=1)\n",
        "\n",
        "class InceptionLastLayer(nn.Module):\n",
        "    def __init__(self,in_channel,layers=[320,384,384,448,384,384,384,192],block=ConvBlock,concat=Concat):\n",
        "        super(InceptionLastLayer,self).__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "        block(in_channel,layers[0],1,1,0),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "        block(in_channel,layers[1],1,1,0),\n",
        "        block(layers[1],layers[2],(1,3),1,(0,1)),\n",
        "        block(layers[2],layers[3],(3,1),1,(1,0)),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "        block(in_channel,layers[4],(3,3),1,1),\n",
        "        concat(layers[4],[layers[5],layers[6]] ),\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "        nn.AvgPool2d(3,1,padding=1),\n",
        "        block(in_channel,layers[7],1,1,0),)\n",
        "    def forward(self,x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "        b4 = self.branch4(x)\n",
        "        return torch.cat([b1,b2,b3,b4],dim=1)\n",
        "\n",
        "\n",
        "Inceptionv3_shape = 224\n",
        "class InceptionV3(nn.Module):\n",
        "    def __init__(self,num_class,block=ConvBlock,base=InceptionV3base,dicount=InceptionV3_discount,base7block=InceptionV3Block,\n",
        "    Substraction=InceptionV3Substraction,lastblock=InceptionLastLayer):\n",
        "        super(InceptionV3,self).__init__()\n",
        "        self.bottle = nn.Sequential(\n",
        "        block(3,32,3,2,0),\n",
        "        block(32,32,3,1,0),\n",
        "        block(32,64,3,1,1),\n",
        "        nn.MaxPool2d(3,2,padding=0),\n",
        "        block(64,80,1,1,0),\n",
        "        block(80,192,3,1,0),\n",
        "        nn.MaxPool2d(3,2,padding=0),\n",
        "        )\n",
        "\n",
        "        self.layer1 = nn.Sequential(base(192))\n",
        "        self.layer2 = nn.Sequential(base(256,[64,48,64,64,96,96,64]))\n",
        "        self.layer3 = nn.Sequential(base(288,[64,48,64,64,96,96,64]))\n",
        "        self.layer4 = nn.Sequential(dicount(288))\n",
        "        self.layer5 = nn.Sequential(base7block(768))\n",
        "        self.layer6 = nn.Sequential(base7block(768,[192,160,160,192,160,160,160,160,192,192]))\n",
        "        self.layer7 = nn.Sequential(base7block(768,[192,160,160,192,160,160,160,160,192,192]))\n",
        "        self.layer8 = nn.Sequential(base7block(768,[192,192,192,192,192,192,192,192,192,192]))\n",
        "        self.layer9 = nn.Sequential(Substraction(768))\n",
        "        self.layer10 = nn.Sequential(\n",
        "        lastblock(1280),)\n",
        "        self.layer11 = nn.Sequential(lastblock(1728))\n",
        "        self.avg = nn.AvgPool2d(8,stride=1,padding=0)\n",
        "        self.fc = nn.Sequential(nn.Dropout(p=0.8),nn.Linear(1728,num_class),)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, (2. / n)**.5)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif  isinstance(m,nn.Linear):\n",
        "                m.weight.data.normal_(0,0.001)\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self,x):\n",
        "        x = self.bottle(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x =self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x =self.layer6(x)\n",
        "        x = self.layer7(x)\n",
        "        x = self.layer8(x)\n",
        "        x =self.layer9(x)\n",
        "        x =self.layer10(x)\n",
        "        x = self.layer11(x)\n",
        "        x = self.avg(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0abfe94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0abfe94",
        "outputId": "dbb860fa-96be-47b4-92cd-23baf1c28e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 149, 149]             864\n",
            "       BatchNorm2d-2         [-1, 32, 149, 149]              64\n",
            "              ReLU-3         [-1, 32, 149, 149]               0\n",
            "         ConvBlock-4         [-1, 32, 149, 149]               0\n",
            "            Conv2d-5         [-1, 32, 147, 147]           9,216\n",
            "       BatchNorm2d-6         [-1, 32, 147, 147]              64\n",
            "              ReLU-7         [-1, 32, 147, 147]               0\n",
            "         ConvBlock-8         [-1, 32, 147, 147]               0\n",
            "            Conv2d-9         [-1, 64, 147, 147]          18,432\n",
            "      BatchNorm2d-10         [-1, 64, 147, 147]             128\n",
            "             ReLU-11         [-1, 64, 147, 147]               0\n",
            "        ConvBlock-12         [-1, 64, 147, 147]               0\n",
            "        MaxPool2d-13           [-1, 64, 73, 73]               0\n",
            "           Conv2d-14           [-1, 80, 73, 73]           5,120\n",
            "      BatchNorm2d-15           [-1, 80, 73, 73]             160\n",
            "             ReLU-16           [-1, 80, 73, 73]               0\n",
            "        ConvBlock-17           [-1, 80, 73, 73]               0\n",
            "           Conv2d-18          [-1, 192, 71, 71]         138,240\n",
            "      BatchNorm2d-19          [-1, 192, 71, 71]             384\n",
            "             ReLU-20          [-1, 192, 71, 71]               0\n",
            "        ConvBlock-21          [-1, 192, 71, 71]               0\n",
            "        MaxPool2d-22          [-1, 192, 35, 35]               0\n",
            "           Conv2d-23           [-1, 64, 35, 35]          12,288\n",
            "      BatchNorm2d-24           [-1, 64, 35, 35]             128\n",
            "             ReLU-25           [-1, 64, 35, 35]               0\n",
            "           Conv2d-26           [-1, 48, 35, 35]           9,216\n",
            "      BatchNorm2d-27           [-1, 48, 35, 35]              96\n",
            "             ReLU-28           [-1, 48, 35, 35]               0\n",
            "           Conv2d-29           [-1, 64, 35, 35]          76,800\n",
            "      BatchNorm2d-30           [-1, 64, 35, 35]             128\n",
            "             ReLU-31           [-1, 64, 35, 35]               0\n",
            "           Conv2d-32           [-1, 64, 35, 35]          12,288\n",
            "      BatchNorm2d-33           [-1, 64, 35, 35]             128\n",
            "             ReLU-34           [-1, 64, 35, 35]               0\n",
            "           Conv2d-35           [-1, 96, 35, 35]          55,296\n",
            "      BatchNorm2d-36           [-1, 96, 35, 35]             192\n",
            "             ReLU-37           [-1, 96, 35, 35]               0\n",
            "           Conv2d-38           [-1, 96, 35, 35]          82,944\n",
            "      BatchNorm2d-39           [-1, 96, 35, 35]             192\n",
            "             ReLU-40           [-1, 96, 35, 35]               0\n",
            "        AvgPool2d-41          [-1, 192, 35, 35]               0\n",
            "           Conv2d-42           [-1, 32, 35, 35]           6,144\n",
            "      BatchNorm2d-43           [-1, 32, 35, 35]              64\n",
            "             ReLU-44           [-1, 32, 35, 35]               0\n",
            "  InceptionV3base-45          [-1, 256, 35, 35]               0\n",
            "           Conv2d-46           [-1, 64, 35, 35]          16,384\n",
            "      BatchNorm2d-47           [-1, 64, 35, 35]             128\n",
            "             ReLU-48           [-1, 64, 35, 35]               0\n",
            "           Conv2d-49           [-1, 48, 35, 35]          12,288\n",
            "      BatchNorm2d-50           [-1, 48, 35, 35]              96\n",
            "             ReLU-51           [-1, 48, 35, 35]               0\n",
            "           Conv2d-52           [-1, 64, 35, 35]          76,800\n",
            "      BatchNorm2d-53           [-1, 64, 35, 35]             128\n",
            "             ReLU-54           [-1, 64, 35, 35]               0\n",
            "           Conv2d-55           [-1, 64, 35, 35]          16,384\n",
            "      BatchNorm2d-56           [-1, 64, 35, 35]             128\n",
            "             ReLU-57           [-1, 64, 35, 35]               0\n",
            "           Conv2d-58           [-1, 96, 35, 35]          55,296\n",
            "      BatchNorm2d-59           [-1, 96, 35, 35]             192\n",
            "             ReLU-60           [-1, 96, 35, 35]               0\n",
            "           Conv2d-61           [-1, 96, 35, 35]          82,944\n",
            "      BatchNorm2d-62           [-1, 96, 35, 35]             192\n",
            "             ReLU-63           [-1, 96, 35, 35]               0\n",
            "        AvgPool2d-64          [-1, 256, 35, 35]               0\n",
            "           Conv2d-65           [-1, 64, 35, 35]          16,384\n",
            "      BatchNorm2d-66           [-1, 64, 35, 35]             128\n",
            "             ReLU-67           [-1, 64, 35, 35]               0\n",
            "  InceptionV3base-68          [-1, 288, 35, 35]               0\n",
            "           Conv2d-69           [-1, 64, 35, 35]          18,432\n",
            "      BatchNorm2d-70           [-1, 64, 35, 35]             128\n",
            "             ReLU-71           [-1, 64, 35, 35]               0\n",
            "           Conv2d-72           [-1, 48, 35, 35]          13,824\n",
            "      BatchNorm2d-73           [-1, 48, 35, 35]              96\n",
            "             ReLU-74           [-1, 48, 35, 35]               0\n",
            "           Conv2d-75           [-1, 64, 35, 35]          76,800\n",
            "      BatchNorm2d-76           [-1, 64, 35, 35]             128\n",
            "             ReLU-77           [-1, 64, 35, 35]               0\n",
            "           Conv2d-78           [-1, 64, 35, 35]          18,432\n",
            "      BatchNorm2d-79           [-1, 64, 35, 35]             128\n",
            "             ReLU-80           [-1, 64, 35, 35]               0\n",
            "           Conv2d-81           [-1, 96, 35, 35]          55,296\n",
            "      BatchNorm2d-82           [-1, 96, 35, 35]             192\n",
            "             ReLU-83           [-1, 96, 35, 35]               0\n",
            "           Conv2d-84           [-1, 96, 35, 35]          82,944\n",
            "      BatchNorm2d-85           [-1, 96, 35, 35]             192\n",
            "             ReLU-86           [-1, 96, 35, 35]               0\n",
            "        AvgPool2d-87          [-1, 288, 35, 35]               0\n",
            "           Conv2d-88           [-1, 64, 35, 35]          18,432\n",
            "      BatchNorm2d-89           [-1, 64, 35, 35]             128\n",
            "             ReLU-90           [-1, 64, 35, 35]               0\n",
            "  InceptionV3base-91          [-1, 288, 35, 35]               0\n",
            "           Conv2d-92          [-1, 384, 17, 17]         995,328\n",
            "      BatchNorm2d-93          [-1, 384, 17, 17]             768\n",
            "             ReLU-94          [-1, 384, 17, 17]               0\n",
            "        ConvBlock-95          [-1, 384, 17, 17]               0\n",
            "           Conv2d-96           [-1, 64, 35, 35]          18,432\n",
            "      BatchNorm2d-97           [-1, 64, 35, 35]             128\n",
            "             ReLU-98           [-1, 64, 35, 35]               0\n",
            "        ConvBlock-99           [-1, 64, 35, 35]               0\n",
            "          Conv2d-100           [-1, 96, 35, 35]          55,296\n",
            "     BatchNorm2d-101           [-1, 96, 35, 35]             192\n",
            "            ReLU-102           [-1, 96, 35, 35]               0\n",
            "       ConvBlock-103           [-1, 96, 35, 35]               0\n",
            "          Conv2d-104           [-1, 96, 17, 17]          82,944\n",
            "     BatchNorm2d-105           [-1, 96, 17, 17]             192\n",
            "            ReLU-106           [-1, 96, 17, 17]               0\n",
            "       ConvBlock-107           [-1, 96, 17, 17]               0\n",
            "       MaxPool2d-108          [-1, 288, 17, 17]               0\n",
            "InceptionV3_discount-109          [-1, 768, 17, 17]               0\n",
            "          Conv2d-110          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-111          [-1, 192, 17, 17]             384\n",
            "            ReLU-112          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-113          [-1, 192, 17, 17]               0\n",
            "          Conv2d-114          [-1, 128, 17, 17]          98,304\n",
            "     BatchNorm2d-115          [-1, 128, 17, 17]             256\n",
            "            ReLU-116          [-1, 128, 17, 17]               0\n",
            "       ConvBlock-117          [-1, 128, 17, 17]               0\n",
            "          Conv2d-118          [-1, 128, 17, 17]         114,688\n",
            "     BatchNorm2d-119          [-1, 128, 17, 17]             256\n",
            "            ReLU-120          [-1, 128, 17, 17]               0\n",
            "       ConvBlock-121          [-1, 128, 17, 17]               0\n",
            "          Conv2d-122          [-1, 192, 17, 17]         172,032\n",
            "     BatchNorm2d-123          [-1, 192, 17, 17]             384\n",
            "            ReLU-124          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-125          [-1, 192, 17, 17]               0\n",
            "          Conv2d-126          [-1, 128, 17, 17]          98,304\n",
            "     BatchNorm2d-127          [-1, 128, 17, 17]             256\n",
            "            ReLU-128          [-1, 128, 17, 17]               0\n",
            "       ConvBlock-129          [-1, 128, 17, 17]               0\n",
            "          Conv2d-130          [-1, 128, 17, 17]         114,688\n",
            "     BatchNorm2d-131          [-1, 128, 17, 17]             256\n",
            "            ReLU-132          [-1, 128, 17, 17]               0\n",
            "       ConvBlock-133          [-1, 128, 17, 17]               0\n",
            "          Conv2d-134          [-1, 128, 17, 17]         114,688\n",
            "     BatchNorm2d-135          [-1, 128, 17, 17]             256\n",
            "            ReLU-136          [-1, 128, 17, 17]               0\n",
            "       ConvBlock-137          [-1, 128, 17, 17]               0\n",
            "          Conv2d-138          [-1, 128, 17, 17]         114,688\n",
            "     BatchNorm2d-139          [-1, 128, 17, 17]             256\n",
            "            ReLU-140          [-1, 128, 17, 17]               0\n",
            "       ConvBlock-141          [-1, 128, 17, 17]               0\n",
            "          Conv2d-142          [-1, 192, 17, 17]         172,032\n",
            "     BatchNorm2d-143          [-1, 192, 17, 17]             384\n",
            "            ReLU-144          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-145          [-1, 192, 17, 17]               0\n",
            "       AvgPool2d-146          [-1, 768, 17, 17]               0\n",
            "          Conv2d-147          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-148          [-1, 192, 17, 17]             384\n",
            "            ReLU-149          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-150          [-1, 192, 17, 17]               0\n",
            "InceptionV3Block-151          [-1, 768, 17, 17]               0\n",
            "          Conv2d-152          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-153          [-1, 192, 17, 17]             384\n",
            "            ReLU-154          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-155          [-1, 192, 17, 17]               0\n",
            "          Conv2d-156          [-1, 160, 17, 17]         122,880\n",
            "     BatchNorm2d-157          [-1, 160, 17, 17]             320\n",
            "            ReLU-158          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-159          [-1, 160, 17, 17]               0\n",
            "          Conv2d-160          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-161          [-1, 160, 17, 17]             320\n",
            "            ReLU-162          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-163          [-1, 160, 17, 17]               0\n",
            "          Conv2d-164          [-1, 192, 17, 17]         215,040\n",
            "     BatchNorm2d-165          [-1, 192, 17, 17]             384\n",
            "            ReLU-166          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-167          [-1, 192, 17, 17]               0\n",
            "          Conv2d-168          [-1, 160, 17, 17]         122,880\n",
            "     BatchNorm2d-169          [-1, 160, 17, 17]             320\n",
            "            ReLU-170          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-171          [-1, 160, 17, 17]               0\n",
            "          Conv2d-172          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-173          [-1, 160, 17, 17]             320\n",
            "            ReLU-174          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-175          [-1, 160, 17, 17]               0\n",
            "          Conv2d-176          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-177          [-1, 160, 17, 17]             320\n",
            "            ReLU-178          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-179          [-1, 160, 17, 17]               0\n",
            "          Conv2d-180          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-181          [-1, 160, 17, 17]             320\n",
            "            ReLU-182          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-183          [-1, 160, 17, 17]               0\n",
            "          Conv2d-184          [-1, 192, 17, 17]         215,040\n",
            "     BatchNorm2d-185          [-1, 192, 17, 17]             384\n",
            "            ReLU-186          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-187          [-1, 192, 17, 17]               0\n",
            "       AvgPool2d-188          [-1, 768, 17, 17]               0\n",
            "          Conv2d-189          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-190          [-1, 192, 17, 17]             384\n",
            "            ReLU-191          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-192          [-1, 192, 17, 17]               0\n",
            "InceptionV3Block-193          [-1, 768, 17, 17]               0\n",
            "          Conv2d-194          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-195          [-1, 192, 17, 17]             384\n",
            "            ReLU-196          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-197          [-1, 192, 17, 17]               0\n",
            "          Conv2d-198          [-1, 160, 17, 17]         122,880\n",
            "     BatchNorm2d-199          [-1, 160, 17, 17]             320\n",
            "            ReLU-200          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-201          [-1, 160, 17, 17]               0\n",
            "          Conv2d-202          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-203          [-1, 160, 17, 17]             320\n",
            "            ReLU-204          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-205          [-1, 160, 17, 17]               0\n",
            "          Conv2d-206          [-1, 192, 17, 17]         215,040\n",
            "     BatchNorm2d-207          [-1, 192, 17, 17]             384\n",
            "            ReLU-208          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-209          [-1, 192, 17, 17]               0\n",
            "          Conv2d-210          [-1, 160, 17, 17]         122,880\n",
            "     BatchNorm2d-211          [-1, 160, 17, 17]             320\n",
            "            ReLU-212          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-213          [-1, 160, 17, 17]               0\n",
            "          Conv2d-214          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-215          [-1, 160, 17, 17]             320\n",
            "            ReLU-216          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-217          [-1, 160, 17, 17]               0\n",
            "          Conv2d-218          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-219          [-1, 160, 17, 17]             320\n",
            "            ReLU-220          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-221          [-1, 160, 17, 17]               0\n",
            "          Conv2d-222          [-1, 160, 17, 17]         179,200\n",
            "     BatchNorm2d-223          [-1, 160, 17, 17]             320\n",
            "            ReLU-224          [-1, 160, 17, 17]               0\n",
            "       ConvBlock-225          [-1, 160, 17, 17]               0\n",
            "          Conv2d-226          [-1, 192, 17, 17]         215,040\n",
            "     BatchNorm2d-227          [-1, 192, 17, 17]             384\n",
            "            ReLU-228          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-229          [-1, 192, 17, 17]               0\n",
            "       AvgPool2d-230          [-1, 768, 17, 17]               0\n",
            "          Conv2d-231          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-232          [-1, 192, 17, 17]             384\n",
            "            ReLU-233          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-234          [-1, 192, 17, 17]               0\n",
            "InceptionV3Block-235          [-1, 768, 17, 17]               0\n",
            "          Conv2d-236          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-237          [-1, 192, 17, 17]             384\n",
            "            ReLU-238          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-239          [-1, 192, 17, 17]               0\n",
            "          Conv2d-240          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-241          [-1, 192, 17, 17]             384\n",
            "            ReLU-242          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-243          [-1, 192, 17, 17]               0\n",
            "          Conv2d-244          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-245          [-1, 192, 17, 17]             384\n",
            "            ReLU-246          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-247          [-1, 192, 17, 17]               0\n",
            "          Conv2d-248          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-249          [-1, 192, 17, 17]             384\n",
            "            ReLU-250          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-251          [-1, 192, 17, 17]               0\n",
            "          Conv2d-252          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-253          [-1, 192, 17, 17]             384\n",
            "            ReLU-254          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-255          [-1, 192, 17, 17]               0\n",
            "          Conv2d-256          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-257          [-1, 192, 17, 17]             384\n",
            "            ReLU-258          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-259          [-1, 192, 17, 17]               0\n",
            "          Conv2d-260          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-261          [-1, 192, 17, 17]             384\n",
            "            ReLU-262          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-263          [-1, 192, 17, 17]               0\n",
            "          Conv2d-264          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-265          [-1, 192, 17, 17]             384\n",
            "            ReLU-266          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-267          [-1, 192, 17, 17]               0\n",
            "          Conv2d-268          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-269          [-1, 192, 17, 17]             384\n",
            "            ReLU-270          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-271          [-1, 192, 17, 17]               0\n",
            "       AvgPool2d-272          [-1, 768, 17, 17]               0\n",
            "          Conv2d-273          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-274          [-1, 192, 17, 17]             384\n",
            "            ReLU-275          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-276          [-1, 192, 17, 17]               0\n",
            "InceptionV3Block-277          [-1, 768, 17, 17]               0\n",
            "          Conv2d-278          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-279          [-1, 192, 17, 17]             384\n",
            "            ReLU-280          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-281          [-1, 192, 17, 17]               0\n",
            "          Conv2d-282            [-1, 320, 8, 8]         552,960\n",
            "     BatchNorm2d-283            [-1, 320, 8, 8]             640\n",
            "            ReLU-284            [-1, 320, 8, 8]               0\n",
            "       ConvBlock-285            [-1, 320, 8, 8]               0\n",
            "          Conv2d-286          [-1, 192, 17, 17]         147,456\n",
            "     BatchNorm2d-287          [-1, 192, 17, 17]             384\n",
            "            ReLU-288          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-289          [-1, 192, 17, 17]               0\n",
            "          Conv2d-290          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-291          [-1, 192, 17, 17]             384\n",
            "            ReLU-292          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-293          [-1, 192, 17, 17]               0\n",
            "          Conv2d-294          [-1, 192, 17, 17]         258,048\n",
            "     BatchNorm2d-295          [-1, 192, 17, 17]             384\n",
            "            ReLU-296          [-1, 192, 17, 17]               0\n",
            "       ConvBlock-297          [-1, 192, 17, 17]               0\n",
            "          Conv2d-298            [-1, 192, 8, 8]         331,776\n",
            "     BatchNorm2d-299            [-1, 192, 8, 8]             384\n",
            "            ReLU-300            [-1, 192, 8, 8]               0\n",
            "       ConvBlock-301            [-1, 192, 8, 8]               0\n",
            "       MaxPool2d-302            [-1, 768, 8, 8]               0\n",
            "InceptionV3Substraction-303           [-1, 1280, 8, 8]               0\n",
            "          Conv2d-304            [-1, 320, 8, 8]         409,600\n",
            "     BatchNorm2d-305            [-1, 320, 8, 8]             640\n",
            "            ReLU-306            [-1, 320, 8, 8]               0\n",
            "       ConvBlock-307            [-1, 320, 8, 8]               0\n",
            "          Conv2d-308            [-1, 384, 8, 8]         491,520\n",
            "     BatchNorm2d-309            [-1, 384, 8, 8]             768\n",
            "            ReLU-310            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-311            [-1, 384, 8, 8]               0\n",
            "          Conv2d-312            [-1, 384, 8, 8]         442,368\n",
            "     BatchNorm2d-313            [-1, 384, 8, 8]             768\n",
            "            ReLU-314            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-315            [-1, 384, 8, 8]               0\n",
            "          Conv2d-316            [-1, 448, 8, 8]         516,096\n",
            "     BatchNorm2d-317            [-1, 448, 8, 8]             896\n",
            "            ReLU-318            [-1, 448, 8, 8]               0\n",
            "       ConvBlock-319            [-1, 448, 8, 8]               0\n",
            "          Conv2d-320            [-1, 384, 8, 8]       4,423,680\n",
            "     BatchNorm2d-321            [-1, 384, 8, 8]             768\n",
            "            ReLU-322            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-323            [-1, 384, 8, 8]               0\n",
            "          Conv2d-324            [-1, 384, 8, 8]         442,368\n",
            "     BatchNorm2d-325            [-1, 384, 8, 8]             768\n",
            "            ReLU-326            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-327            [-1, 384, 8, 8]               0\n",
            "          Conv2d-328            [-1, 384, 8, 8]         442,368\n",
            "     BatchNorm2d-329            [-1, 384, 8, 8]             768\n",
            "            ReLU-330            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-331            [-1, 384, 8, 8]               0\n",
            "          Concat-332            [-1, 768, 8, 8]               0\n",
            "       AvgPool2d-333           [-1, 1280, 8, 8]               0\n",
            "          Conv2d-334            [-1, 192, 8, 8]         245,760\n",
            "     BatchNorm2d-335            [-1, 192, 8, 8]             384\n",
            "            ReLU-336            [-1, 192, 8, 8]               0\n",
            "       ConvBlock-337            [-1, 192, 8, 8]               0\n",
            "InceptionLastLayer-338           [-1, 1728, 8, 8]               0\n",
            "          Conv2d-339            [-1, 320, 8, 8]         552,960\n",
            "     BatchNorm2d-340            [-1, 320, 8, 8]             640\n",
            "            ReLU-341            [-1, 320, 8, 8]               0\n",
            "       ConvBlock-342            [-1, 320, 8, 8]               0\n",
            "          Conv2d-343            [-1, 384, 8, 8]         663,552\n",
            "     BatchNorm2d-344            [-1, 384, 8, 8]             768\n",
            "            ReLU-345            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-346            [-1, 384, 8, 8]               0\n",
            "          Conv2d-347            [-1, 384, 8, 8]         442,368\n",
            "     BatchNorm2d-348            [-1, 384, 8, 8]             768\n",
            "            ReLU-349            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-350            [-1, 384, 8, 8]               0\n",
            "          Conv2d-351            [-1, 448, 8, 8]         516,096\n",
            "     BatchNorm2d-352            [-1, 448, 8, 8]             896\n",
            "            ReLU-353            [-1, 448, 8, 8]               0\n",
            "       ConvBlock-354            [-1, 448, 8, 8]               0\n",
            "          Conv2d-355            [-1, 384, 8, 8]       5,971,968\n",
            "     BatchNorm2d-356            [-1, 384, 8, 8]             768\n",
            "            ReLU-357            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-358            [-1, 384, 8, 8]               0\n",
            "          Conv2d-359            [-1, 384, 8, 8]         442,368\n",
            "     BatchNorm2d-360            [-1, 384, 8, 8]             768\n",
            "            ReLU-361            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-362            [-1, 384, 8, 8]               0\n",
            "          Conv2d-363            [-1, 384, 8, 8]         442,368\n",
            "     BatchNorm2d-364            [-1, 384, 8, 8]             768\n",
            "            ReLU-365            [-1, 384, 8, 8]               0\n",
            "       ConvBlock-366            [-1, 384, 8, 8]               0\n",
            "          Concat-367            [-1, 768, 8, 8]               0\n",
            "       AvgPool2d-368           [-1, 1728, 8, 8]               0\n",
            "          Conv2d-369            [-1, 192, 8, 8]         331,776\n",
            "     BatchNorm2d-370            [-1, 192, 8, 8]             384\n",
            "            ReLU-371            [-1, 192, 8, 8]               0\n",
            "       ConvBlock-372            [-1, 192, 8, 8]               0\n",
            "InceptionLastLayer-373           [-1, 1728, 8, 8]               0\n",
            "       AvgPool2d-374           [-1, 1728, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 27,452,896\n",
            "Trainable params: 27,452,896\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.02\n",
            "Forward/backward pass size (MB): 298.40\n",
            "Params size (MB): 104.72\n",
            "Estimated Total Size (MB): 404.15\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "IncV3 = InceptionV3(2).to(device)\n",
        "summary(IncV3,(3,299,299))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d17782a-45c9-412b-9344-a45ac363c5d9",
      "metadata": {
        "id": "1d17782a-45c9-412b-9344-a45ac363c5d9"
      },
      "outputs": [],
      "source": [
        "#lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387a6640",
      "metadata": {
        "id": "387a6640"
      },
      "outputs": [],
      "source": [
        "layer=nn.LSTM(input_size=10,\n",
        "            hidden_size=3,\n",
        "            num_layers=1,\n",
        "            batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7411a80f-3552-4a74-b2bc-1c21187e03ba",
      "metadata": {
        "id": "7411a80f-3552-4a74-b2bc-1c21187e03ba",
        "outputId": "417fbbcd-f103-42aa-8311-a9766ced9998"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0654, -0.3733, -0.0991,  0.1919, -0.4678, -0.3454,  0.5222,  0.3522,\n",
              "         -0.0755, -0.3313],\n",
              "        [-0.3000,  0.3332, -0.5111,  0.1441, -0.2760,  0.0533, -0.0831,  0.2240,\n",
              "          0.2663, -0.3477],\n",
              "        [-0.2603, -0.2722, -0.1456,  0.1814,  0.0043, -0.1016,  0.1311, -0.0484,\n",
              "          0.4457, -0.2364],\n",
              "        [ 0.1712,  0.4973,  0.4262, -0.2991, -0.1670, -0.3681, -0.5424,  0.4758,\n",
              "         -0.2123,  0.5017],\n",
              "        [ 0.2426, -0.5633, -0.1255,  0.2236, -0.5099,  0.1872, -0.0861,  0.1386,\n",
              "         -0.1111,  0.2494],\n",
              "        [ 0.3052,  0.5150,  0.0707, -0.1595,  0.2952, -0.1881, -0.2803,  0.0662,\n",
              "          0.5578, -0.1490],\n",
              "        [-0.4484, -0.0893, -0.2692,  0.3828, -0.1572, -0.3873,  0.4426, -0.2759,\n",
              "         -0.3151,  0.5624],\n",
              "        [ 0.3864,  0.4037, -0.0150, -0.1108, -0.1606,  0.0961, -0.3214, -0.2753,\n",
              "         -0.0631,  0.4536],\n",
              "        [ 0.0504, -0.0093,  0.0967, -0.0810,  0.2669,  0.3659,  0.3039, -0.2347,\n",
              "         -0.3754,  0.3021],\n",
              "        [-0.0837, -0.2032,  0.5135,  0.5556,  0.1246,  0.3575,  0.4695, -0.2085,\n",
              "          0.4396, -0.3672],\n",
              "        [-0.3521, -0.2566, -0.4035,  0.1588,  0.3462,  0.2909,  0.3325,  0.0007,\n",
              "         -0.1755,  0.3766],\n",
              "        [ 0.5029,  0.2431, -0.1413, -0.1073, -0.3159,  0.0671,  0.4832,  0.3104,\n",
              "          0.0691, -0.5409]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layer.state_dict()[]['weight_ih_l0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbd74b7-00bd-4f8f-b030-7e7ab47ddb83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbbd74b7-00bd-4f8f-b030-7e7ab47ddb83",
        "outputId": "8aa24f21-3a21-4cbe-9a46-33a391296fc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([12, 10]), torch.Size([12, 3]), torch.Size([12]), torch.Size([12])]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "[layer.state_dict()[i].shape for i in list(layer.state_dict())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff832033-6bce-45c2-b0c1-9ef121c08a14",
      "metadata": {
        "id": "ff832033-6bce-45c2-b0c1-9ef121c08a14"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X=np.array([[84345856,29446272],[41495552,23508032],[39112400,27452896]])\n",
        "y=np.array([190.71,193.87,191.56])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2c752d-b7dd-4c25-9832-edc200963e42",
      "metadata": {
        "id": "ee2c752d-b7dd-4c25-9832-edc200963e42"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression as LR\n",
        "lr=LR().fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28db9e16-5036-4b0a-ae3a-e2fc0261c24d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28db9e16-5036-4b0a-ae3a-e2fc0261c24d",
        "outputId": "ac3f0572-d141-4d0a-ea39-82ac5417cb04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6.83204197e-09, -5.81444178e-07])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "lr.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e5e48a-e471-4bb4-85af-1d716709bf59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14e5e48a-e471-4bb4-85af-1d716709bf59",
        "outputId": "aa43c1e3-0262-48bd-e029-8752b45e9258"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "207.255108988361"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "lr.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ebe3a9-64ac-4b2c-8684-5c0992c9d8da",
      "metadata": {
        "id": "46ebe3a9-64ac-4b2c-8684-5c0992c9d8da"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "代码.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}